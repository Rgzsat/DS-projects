DECISION TREE: The Decision Tree for a classification problem has been coded,
specifically using a binary approach.

Step 1: The loss functions are initialized, which are the Entropy and Gini impurity
Step 2: The information gain, split for a maximum information gain, and best split, are the main functions in this step.
Step 3: Split execution, prediction, and training of the tree are the 3 implemented functions in the code. 
Step 4: The binary classifier is initially coded for predictions and execution of cross validation.


STOCHASTIC GRADIENT DESCENT (SGD):The Online Gradient Descent has been implemented based on the supervised case. 
Initially, the Mean Squared Error (MSE) is defined as the loss function, which has been coded and compared with the implementation of the scikit-learn library.

ADABOOST: Implementation of the algorithm is divided into 3 steps, explained as follows:

1) Initial or helper functions: In this case, 3 functions are coded to initialize the algorithm, 
the first calculates the error based on the target and predicted variable, the second indicates 
the weight of a weak classifier  using the majority vote of the final classifier, 
the third updates individual weights per each boosting iteration.

2) AdaBosst function: It executes the boosting taking the matrix of features,
output variable, and desired number of boosting iterations. 
In this specific example, the Decision tree classifier has been selected.

3) Predictions: A final function that takes the independent variables as an input argument is coded.
Inside the function, the weak classifier is iterated over the number of selected boosting
rounds and weighted by alpha parameter.
